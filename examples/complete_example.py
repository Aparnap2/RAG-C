"""Complete example showcasing all UNI-RAG features."""

import asyncio
from uni_rag import RAGPipeline, RAGConfig, RAGQuery

async def main():
    # Complete configuration
    config = RAGConfig(
        # MCP configuration
        mcp={
            "servers": {
                "llm_server": {
                    "transport": "stdio",
                    "command": "python -m llm_mcp_server"
                }
            },
            "tenants": {
                "demo": {"allowed_tools": ["llm.generate"]}
            }
        },
        
        # Storage configuration
        vector_store={
            "provider": "qdrant",
            "url": "http://localhost:6333",
            "collection_name": "uni_rag_docs"
        },
        text_index={
            "provider": "opensearch",
            "host": "localhost",
            "port": 9200
        },
        
        # Embeddings
        embedding={
            "provider": "google_genai",
            "api_key": "your-api-key"
        },
        
        # LLM
        llm={
            "provider": "google_genai",
            "model": "gemini-1.5-pro",
            "google_api_key": "your-api-key"
        },
        
        # Advanced processing
        chunking={
            "chunk_sizes": [200, 400, 800],
            "overlap_ratio": 0.15
        },
        
        # LangGraph orchestration
        llm_orchestration={
            "llm_tool": "llm.generate",
            "enable_preprocessing": True,
            "retrieval_k": 50,
            "final_k": 5,
            "quality_threshold": 0.75
        },
        
        # Enhanced reranking
        reranker={
            "threshold": 0.8,
            "recency_weight": 0.1,
            "entity_weight": 0.2
        },
        
        # Ingestion configuration
        ingestion={
            "file": {
                "use_docling": True,
                "max_concurrent": 5
            },
            "web": {
                "use_crawl4ai": True,
                "max_concurrent": 3,
                "timeout": 30
            }
        }
    )
    
    # Initialize pipeline
    pipeline = RAGPipeline(config)
    
    print("üöÄ UNI-RAG Complete Example")
    print("=" * 50)
    
    # 1. Ingest mixed sources
    print("\nüì• Ingesting mixed sources...")
    sources = {
        "files": ["documents/report.pdf", "documents/manual.docx"],
        "urls": ["https://example.com/article", "https://example.com/blog"],
        "directories": ["documents/research/"]
    }
    
    try:
        documents = await pipeline.ingest_mixed_sources(
            sources,
            metadata={"project": "demo", "priority": "high"}
        )
        print(f"‚úÖ Ingested {len(documents)} documents")
    except Exception as e:
        print(f"‚ùå Ingestion failed: {e}")
        # Continue with example query
    
    # 2. Single source ingestion examples
    print("\nüìÑ Single source examples...")
    
    # File ingestion
    try:
        docs = await pipeline.ingest("example.pdf", "file")
        print(f"‚úÖ File: {len(docs)} documents")
    except Exception as e:
        print(f"‚ö†Ô∏è  File ingestion: {e}")
    
    # URL ingestion
    try:
        docs = await pipeline.ingest("https://example.com", "url")
        print(f"‚úÖ URL: {len(docs)} documents")
    except Exception as e:
        print(f"‚ö†Ô∏è  URL ingestion: {e}")
    
    # Auto-detection
    try:
        docs = await pipeline.ingest("documents/", "auto")  # Auto-detects directory
        print(f"‚úÖ Auto-detect: {len(docs)} documents")
    except Exception as e:
        print(f"‚ö†Ô∏è  Auto-detect: {e}")
    
    # 3. Advanced querying
    print("\nüîç Advanced querying with LangGraph...")
    
    queries = [
        "What are the key benefits of artificial intelligence?",
        "How does machine learning improve business processes?",
        "What are the latest trends in technology?"
    ]
    
    for i, query_text in enumerate(queries, 1):
        print(f"\nü§î Query {i}: {query_text}")
        
        try:
            query = RAGQuery(
                query=query_text,
                max_results=5,
                source_type="mixed"
            )
            
            response = await pipeline.query(query)
            
            print(f"üí° Answer: {response.answer[:200]}...")
            print(f"üìä Context length: {len(response.context)} chars")
            print(f"üìö Sources: {len(response.sources)}")
            
        except Exception as e:
            print(f"‚ùå Query failed: {e}")
    
    # 4. Demonstrate different ingestion methods
    print("\nüîß Advanced ingestion examples...")
    
    # Sitemap ingestion
    try:
        docs = await pipeline.ingest("https://example.com/sitemap.xml", "sitemap")
        print(f"‚úÖ Sitemap: {len(docs)} documents")
    except Exception as e:
        print(f"‚ö†Ô∏è  Sitemap: {e}")
    
    print("\nüéâ Complete example finished!")
    print("=" * 50)
    print("Features demonstrated:")
    print("‚úì Multi-chunk strategies (200/400/800 tokens)")
    print("‚úì LangGraph orchestration with 6-stage workflow")
    print("‚úì Docling document processing")
    print("‚úì Crawl4AI web scraping")
    print("‚úì Unified ingestion (files, URLs, directories)")
    print("‚úì Hybrid retrieval with RRF")
    print("‚úì Cross-encoder reranking")
    print("‚úì MCP-first architecture")

if __name__ == "__main__":
    asyncio.run(main())